import numpy as np
from scipy.sparse import diags
import yt
from tqdm import tqdm
import os
from multiprocessing import Pool, cpu_count
from functools import partial

# Configuración global
GAMMA = 1.354
MACH = 1.7
FIELDS = ["density", "x_velocity", "y_velocity", "Temp"]
ROI_PARAMS = {
    'threshold': 100.0,
    'window_size': 5,
    'preset': 75,
    'offset': 1125
}
MAX_CORES = 64  # Ajustar según disponibilidad real
CHUNK_SIZE = 8  # Tamaño de chunk para multiprocessing

def process_single_snapshot(file_info):
    """Procesa un solo snapshot para cálculo de promedios con ROI"""
    file, root_path, max_level, dims = file_info
    try:
        ds = yt.load(os.path.join(root_path, file))
        cube = ds.covering_grid(max_level, left_edge=ds.domain_left_edge, 
                              dims=dims, fields=[("boxlib", f) for f in FIELDS])
        
        # Detección dinámica de ROI
        temp = cube[("boxlib", "Temp")].d[:, :, 0]
        profile = temp[0, :]
        diff_avg = np.convolve(np.diff(profile), 
                             np.ones(2*ROI_PARAMS['window_size']+1)/(2*ROI_PARAMS['window_size']+1), 
                             mode='same')
        base_candidates = np.where(diff_avg > ROI_PARAMS['threshold'])[0]
        
        base_index = dims[1] // 2 if len(base_candidates) == 0 else base_candidates[0]
        lower_bound = max(0, base_index - ROI_PARAMS['preset'])
        upper_bound = min(dims[1], base_index + ROI_PARAMS['offset'])
        
        # Extraer ROI para cada campo
        roi_data = {}
        for field in FIELDS:
            roi = cube[("boxlib", field)].d[:, lower_bound:upper_bound, 0]
            roi_data[field] = roi
            
        return roi_data
        
    except Exception as e:
        print(f"Error procesando {file}: {str(e)}")
        return None

def calculate_mean_fields_parallel(snapshot_files, root_path, max_level=2):
    """Calcula campos medios con ROI usando multiprocessing"""
    first_ds = yt.load(os.path.join(root_path, snapshot_files[0]))
    ref = int(np.prod(first_ds.ref_factors[0:max_level]))
    dims = first_ds.domain_dimensions * ref
    
    # Configuración de paralelización
    n_workers = min(MAX_CORES, cpu_count(), len(snapshot_files))
    print(f"Usando {n_workers} cores para procesamiento paralelo")
    
    # Procesamiento paralelo
    with Pool(n_workers) as pool:
        file_infos = [(f, root_path, max_level, dims) for f in snapshot_files]
        results = list(tqdm(pool.imap(process_single_snapshot, file_infos, chunksize=CHUNK_SIZE),
                      total=len(snapshot_files), desc="Procesando snapshots"))
    
    # Filtrar resultados válidos
    valid_results = [r for r in results if r is not None]
    if not valid_results:
        raise ValueError("No se procesaron snapshots válidos")
    
    # Determinar tamaño máximo de ROI
    max_ny = max(r["density"].shape[1] for r in valid_results)
    
    # Calcular promedios con padding
    mean_fields = {}
    for field in FIELDS:
        padded_arrays = []
        for result in valid_results:
            roi = result[field]
            if roi.shape[1] < max_ny:
                pad_width = ((0, 0), (0, max_ny - roi.shape[1]))
                roi = np.pad(roi, pad_width, mode='constant', constant_values=0)
            padded_arrays.append(roi)
        
        mean_fields[field] = np.mean(padded_arrays, axis=0)
    
    # Calcular volumen de celda
    if 'dx' in dir(first_ds):
        dV = first_ds.domain_width[0]/dims[0] * first_ds.domain_width[1]/dims[1] * first_ds.domain_width[2]/dims[2]
    else:
        dV = np.prod(first_ds.domain_width / first_ds.domain_dimensions)
    
    return mean_fields, dV, mean_fields["density"].shape

def build_compressible_W_optimized(mean_fields, cell_volume, dims):
    """Versión optimizada para construcción de W"""
    nx, ny = dims
    n_points = nx * ny
    
    # Pre-calcular términos comunes
    rho_mean = mean_fields["density"].flatten()
    T_mean = mean_fields["Temp"].flatten()
    gamma_M2 = GAMMA * MACH**2
    common_denominator = GAMMA * (GAMMA-1) * T_mean * MACH**2
    
    # Construir diagonal directamente con operaciones vectorizadas
    diagonal = np.empty(4 * n_points)
    diagonal[0::4] = T_mean / (gamma_M2 * rho_mean)  # ρ
    diagonal[1::4] = rho_mean                        # vx
    diagonal[2::4] = rho_mean                        # vy
    diagonal[3::4] = rho_mean / common_denominator   # T
    
    # Aplicar volumen de celda
    diagonal *= cell_volume
    
    return diags(diagonal, 0, format='csr')

if __name__ == "__main__":
    # Configuración de rutas
    root = "../../../Simulations/S0.1"
    output_dir = "W_matrices"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. Cargar y ordenar snapshots
    files = sorted([f for f in os.listdir(root) if "pltAMR2NSW_" in f], 
                  key=lambda x: int(x.split("_")[-1]))
    
    # 2. Procesamiento paralelo de promedios
    print("\nCalculando campos medios con ROI dinámico...")
    mean_fields, dV, dims = calculate_mean_fields_parallel(files, root)
    
    # 3. Construcción optimizada de W
    print("\nConstruyendo matriz W compresible...")
    W = build_compressible_W_optimized(mean_fields, dV, dims)
    
    # 4. Guardar resultados
    output_path = os.path.join(output_dir, f"W_compressible_gamma{GAMMA}_Mach{MACH}_ROI.npz")
    from scipy.sparse import save_npz
    save_npz(output_path, W)
    
    print(f"\nMatriz W guardada en {output_path}")
    print(f"Dimensiones: {W.shape}")
    print(f"Elementos no cero: {W.count_nonzero():,}")
    print(f"Memoria aproximada: {W.data.nbytes/1e6:.2f} MB")
